import json
import argparse
import os
import random
from typing import List, Dict, Any, Optional

def load_original_data(input_path: str) -> List[Dict[str, Any]]:
    """åŠ è½½JSONLæ ¼å¼åŸå§‹æ•°æ®ï¼Œè¿‡æ»¤æ— æ•ˆè¡Œï¼ˆéå¤§æ‹¬å·èµ·å§‹ã€è§£æå¤±è´¥ã€å­—æ®µç¼ºå¤±ï¼‰"""
    original_data = []
    valid_lines = 0
    invalid_lines = 0

    try:
        with open(input_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                cleaned_line = line.strip()
                
                # è¿‡æ»¤ç©ºè¡Œ
                if not cleaned_line:
                    invalid_lines += 1
                    continue
                
                # è¿‡æ»¤éå¤§æ‹¬å·èµ·å§‹çš„è¡Œ
                if not cleaned_line.startswith('{'):
                    print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡Œéå¤§æ‹¬å·èµ·å§‹ï¼Œå·²å¿½ç•¥ -> å†…å®¹ï¼š{cleaned_line[:50]}...")
                    invalid_lines += 1
                    continue
                
                # è§£æJSON
                try:
                    data_item = json.loads(cleaned_line)
                except json.JSONDecodeError as e:
                    print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡ŒJSONæ ¼å¼é”™è¯¯ï¼Œå·²å¿½ç•¥ -> é”™è¯¯ï¼š{str(e)[:50]}")
                    invalid_lines += 1
                    continue
                
                # æ ¡éªŒå¿…è¦å­—æ®µ
                required_fields = ["A", "Q_chain", "context"]
                if not all(field in data_item for field in required_fields):
                    print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡Œç¼ºå¤±å¿…è¦å­—æ®µï¼ˆA/Q_chain/contextï¼‰ï¼Œå·²å¿½ç•¥")
                    invalid_lines += 1
                    continue
                
                # æ ¡éªŒcontextä¸­çš„å¿…è¦å­—æ®µ
                context = data_item["context"]
                required_context_fields = ["R1", "R2", "R3", "connection_d1_d2", "connection_d2_d3"]
                if not all(field in context for field in required_context_fields):
                    print(f"è­¦å‘Šï¼šç¬¬{line_num}è¡Œcontextç¼ºå¤±å¿…è¦å­—æ®µï¼ˆR1/R2/R3/connection_d1_d2/connection_d2_d3ï¼‰ï¼Œå·²å¿½ç•¥")
                    invalid_lines += 1
                    continue
                
                original_data.append(data_item)
                valid_lines += 1

        print(f"\næ•°æ®åŠ è½½å®Œæˆï¼š")
        print(f"- æ€»è¡Œæ•°ï¼š{line_num}")
        print(f"- æœ‰æ•ˆè¡Œæ•°ï¼ˆåŸºç¡€è¿‡æ»¤åï¼‰ï¼š{valid_lines}")
        print(f"- æ— æ•ˆè¡Œæ•°ï¼ˆæ ¼å¼/å­—æ®µé—®é¢˜ï¼‰ï¼š{invalid_lines}")
        return original_data

    except FileNotFoundError:
        raise FileNotFoundError(f"æœªæ‰¾åˆ°è¾“å…¥æ–‡ä»¶ï¼š{input_path}")
    except Exception as e:
        raise RuntimeError(f"åŠ è½½æ–‡ä»¶æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯ï¼š{str(e)}")

def is_answer_in_question(answer: str, question: str) -> bool:
    """åˆ¤æ–­answeræ˜¯å¦å®Œæ•´å‡ºç°åœ¨questionä¸­ï¼ˆå»å‰åç©ºæ ¼ååŒ¹é…ï¼‰"""
    if not answer or not question:
        return False  # ä»»ä¸€ä¸ºç©ºï¼Œä¸è§†ä¸ºåŒ¹é…
    # å»å‰åç©ºæ ¼ååˆ¤æ–­å­ä¸²åŒ…å«ï¼ˆå®Œæ•´å‡ºç°ï¼‰
    clean_answer = answer.strip()
    clean_question = question.strip()
    return clean_answer in clean_question

def filter_invalid_data(original_data: List[Dict[str, Any]]) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    è¿‡æ»¤æ— æ•ˆæ•°æ®ï¼š
    1. Answerå®Œæ•´å‡ºç°åœ¨ä»»ä½•ä¸€è½®Questionä¸­
    2. ä»»ä½•ä¸€è½®QuestionåŒ…å«æ¨¡ç³Šå…³é”®è¯ï¼ˆå‘¨å›´ã€é™„è¿‘ã€ä¹‹ä¸€ç­‰ï¼‰
    è¿”å›ï¼š(æœ‰æ•ˆæ•°æ®åˆ—è¡¨, æ— æ•ˆæ•°æ®åˆ—è¡¨)
    """
    filtered_data = []
    invalid_data = []
    
    # å®šä¹‰éœ€è¦è¿‡æ»¤çš„æ¨¡ç³Šå…³é”®è¯ï¼ˆåŠå…¶ç›¸ä¼¼è¡¨è¿°ï¼‰
    VAGUE_KEYWORDS = ["å‘¨å›´", "é™„è¿‘", "ä¹‹ä¸€", "æ—è¾¹", "é‚»è¿‘"]

    for idx, item in enumerate(original_data, 1):
        answer = item.get("A", "").strip()
        q_chain = item.get("Q_chain", [])
        
        # åŸºç¡€æ£€æŸ¥ï¼šå¿…é¡»å­˜åœ¨ level=2 çš„ question æ‰èƒ½ä½œä¸ºæœ‰æ•ˆæ•°æ®çš„åŸºç¡€
        has_level2 = False
        for q in q_chain:
            if q.get("level") == 2:
                has_level2 = True
                break
        
        if not has_level2:
            print(f"è­¦å‘Šï¼šç¬¬{idx}æ¡æ•°æ®æœªæ‰¾åˆ°level2çš„questionï¼Œå·²è·³è¿‡")
            continue

        # --- æ–°å¢è¿‡æ»¤é€»è¾‘ ---
        is_invalid = False
        invalid_reason = ""

        for q_item in q_chain:
            question_text = q_item.get("question", "").strip()
            level = q_item.get("level")
            
            # è§„åˆ™ 1: Answer å‡ºç°åœ¨è¯¥è½® Question ä¸­
            if is_answer_in_question(answer, question_text):
                is_invalid = True
                invalid_reason = f"Answerå®Œæ•´å‡ºç°åœ¨level={level}çš„Questionä¸­"
                break
            
            # è§„åˆ™ 2: è¯¥è½® Question åŒ…å«æ¨¡ç³Šå…³é”®è¯
            for kw in VAGUE_KEYWORDS:
                if kw in question_text:
                    is_invalid = True
                    invalid_reason = f"Question(level={level})åŒ…å«æ¨¡ç³Šå…³é”®è¯'{kw}'"
                    break
            
            if is_invalid:
                break # åªè¦å‘ç°ä¸€ä¸ªé—®é¢˜æ»¡è¶³æ¡ä»¶ï¼Œæ•´ç»„æ•°æ®å³æ— æ•ˆ
        
        if is_invalid:
            item["invalid_reason"] = invalid_reason
            item["invalid_data_source"] = input_path.split('/')[-2]
            invalid_data.append(item)
            print(f"è¿‡æ»¤æ— æ•ˆæ•°æ®ï¼šç¬¬{idx}æ¡ -> {invalid_reason}")
        else:
            filtered_data.append(item)
    
    print(f"\næ•°æ®è¿‡æ»¤å®Œæˆï¼š")
    print(f"- è¿‡æ»¤åæœ‰æ•ˆæ•°æ®æ•°ï¼š{len(filtered_data)}")
    print(f"- æ— æ•ˆæ•°æ®æ•°ï¼š{len(invalid_data)}")
    return filtered_data, invalid_data

def save_invalid_data(invalid_data: List[Dict[str, Any]], invalid_output_path: str):
    """ä¿å­˜æ— æ•ˆæ•°æ®åˆ°ç‹¬ç«‹JSONLæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰"""
    if not invalid_data:
        return
    
    # è¿½åŠ å†™å…¥JSONLæ ¼å¼
    with open(invalid_output_path, 'a+', encoding='utf-8') as f:
        for record in invalid_data:
            json.dump(record, f, ensure_ascii=False)
            f.write('\n')
    
    # ç»Ÿè®¡æ— æ•ˆæ–‡ä»¶æ€»è®°å½•æ•°
    total_invalid = sum(1 for line in open(invalid_output_path, 'r', encoding='utf-8') if line.strip())
    print(f"\næ— æ•ˆæ•°æ®å·²è¿½åŠ è‡³ï¼š{invalid_output_path}")
    print(f"æ— æ•ˆæ–‡ä»¶å½“å‰æ€»è®°å½•æ•°ï¼š{total_invalid}")

def get_max_existing_r_count(reference_path: str) -> int:
    """è¯»å–JSONLæ ¼å¼å‚è€ƒæ–‡ä»¶çš„æœ‰æ•ˆè¡Œæ•°ï¼ˆå³å·²æœ‰çš„Ræ€»æ•°ï¼‰ï¼Œä½œä¸ºèµ·å§‹ç¼–å·ä¾æ®"""
    max_count = 0
    if os.path.exists(reference_path):
        try:
            with open(reference_path, 'r', encoding='utf-8') as f:
                for line in f:
                    cleaned_line = line.strip()
                    if not cleaned_line:
                        continue
                    try:
                        json.loads(cleaned_line)
                        max_count += 1
                    except json.JSONDecodeError:
                        continue
            print(f"å‚è€ƒæ–‡ä»¶å·²å­˜åœ¨ {max_count} ä¸ªRè®°å½•ï¼ˆæœ€å¤§ç¼–å·ï¼š{max_count}ï¼‰")
        except Exception as e:
            print(f"è¯»å–å‚è€ƒæ–‡ä»¶å¤±è´¥ï¼š{str(e)}ï¼Œå°†ä»1å¼€å§‹åˆ†é…Rç¼–å·")
            max_count = 0
    else:
        print(f"å‚è€ƒæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†ä»1å¼€å§‹åˆ†é…Rç¼–å·")
    return max_count

def format_connection(connection: Dict[str, Any]) -> Dict[str, Any]:
    """æ ¼å¼åŒ–connectionå­—æ®µï¼šç§»é™¤edit_distanceï¼Œsimilarityä¿ç•™ä¸¤ä½å°æ•°"""
    formatted = {}
    if "bridge" in connection:
        formatted["bridge"] = connection["bridge"]
    if "similarity" in connection:
        try:
            similarity = float(connection["similarity"])
            formatted["similarity"] = round(similarity, 2)
        except (ValueError, TypeError):
            formatted["similarity"] = connection["similarity"]
    return formatted

def process_r_reference(original_data: List[Dict[str, Any]], start_r_id: int) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    å¤„ç†Ræ•°æ®å’Œç¼–å·åˆ†é…ï¼ˆä»start_r_idå¼€å§‹ï¼‰
    åŒ…å«åŠŸèƒ½ï¼š
    1. Rå†…å®¹å‰æ‹¼æ¥Title
    2. æå–Level 0/1é—®é¢˜åˆ°Metadata
    """
    current_new_r_id = start_r_id
    reference_records = []
    data_r_mapping = []

    for idx, item in enumerate(original_data):
        context = item["context"]
        r_ids = []

        # è¾…åŠ©å‡½æ•°ï¼šç”Ÿæˆå¸¦æ ‡é¢˜çš„å†…å®¹
        def format_r_content(doc_key, r_key):
            doc_info = context.get(doc_key, {})
            title = doc_info.get("title", "").strip()
            raw_content = context[r_key][0] if context[r_key] and isinstance(context[r_key], list) else str(context[r_key])
            
            if title:
                return f"{title}\n{raw_content}", title, doc_info.get("id", "")
            else:
                return raw_content, title, doc_info.get("id", "")

        # --- å¤„ç†R1, R2, R3 ---
        for doc_key, r_key, r_type in [
            ("document1", "R1", "R1"),
            ("document2", "R2", "R2"),
            ("document3", "R3", "R3")
        ]:
            val, title, doc_id = format_r_content(doc_key, r_key)
            record = {
                "value": val,
                "meta_data": {
                    "r_type": r_type,
                    "document_id": doc_id,
                    "document_title": title,
                    "original_data_source": input_path.split('/')[-2]
                }
            }
            reference_records.append(record)
            r_ids.append(current_new_r_id)
            current_new_r_id += 1

        data_r_mapping.append(r_ids)

    # ç”Ÿæˆé—®é¢˜æ–‡ä»¶æ•°æ®
    questions_data = []
    for idx, item in enumerate(original_data):
        # æå–å„çº§é—®é¢˜
        question_lvl_2 = None
        question_lvl_1 = None
        question_lvl_0 = None
        
        for q in item["Q_chain"]:
            level = q.get("level")
            if level == 2:
                question_lvl_2 = q["question"]
            elif level == 1:
                question_lvl_1 = q["question"]
            elif level == 0:
                question_lvl_0 = q["question"]
        
        if not question_lvl_2:
            continue
        
        context = item["context"]
        connection_d1_d2 = format_connection(context.get("connection_d1_d2", {}))
        connection_d2_d3 = format_connection(context.get("connection_d2_d3", {}))
        
        questions_data.append({
            "answer": item["A"],
            "question": question_lvl_2,
            "label": data_r_mapping[idx],
            "meta_data": {
                "connection_d1_d2": connection_d1_d2,
                "connection_d2_d3": connection_d2_d3,
                "question_level_1": question_lvl_1,
                "question_level_0": question_lvl_0,
                "original_data_source": input_path.split('/')[-2]
            }
        })

    print(f"\nRæ•°æ®å¤„ç†å®Œæˆï¼š")
    print(f"- æ–°å¢Rè®°å½•æ•°ï¼š{len(reference_records)}")
    print(f"- ç”Ÿæˆæœ‰æ•ˆé—®é¢˜æ•°æ®æ•°ï¼š{len(questions_data)}")
    print(f"- æœ¬æ¬¡åˆ†é…Rç¼–å·èŒƒå›´ï¼š{start_r_id} ~ {current_new_r_id - 1}")
    return questions_data, reference_records

def sample_random_records(records: List[Dict[str, Any]], sample_size: int = 3) -> List[Dict[str, Any]]:
    """éšæœºæŠ½æ ·ç”¨äºé‡å¤æ£€æµ‹"""
    if len(records) <= sample_size:
        return records.copy()
    return random.sample(records, sample_size)

def check_duplicate_qa(qa_path: str, sample_records: List[Dict[str, Any]]) -> bool:
    """æ£€æŸ¥æŠ½æ ·æ•°æ®æ˜¯å¦å·²å­˜åœ¨"""
    if not os.path.exists(qa_path):
        return False
    
    sample_keys = []
    for record in sample_records:
        key = (record.get("question", "").strip(), record.get("answer", "").strip())
        sample_keys.append(key)
    
    try:
        with open(qa_path, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                cleaned_line = line.strip()
                if not cleaned_line:
                    continue
                try:
                    existing = json.loads(cleaned_line)
                    existing_key = (existing.get("question", "").strip(), existing.get("answer", "").strip())
                    if existing_key in sample_keys:
                        print(f"âŒ å‘ç°é‡å¤æ•°æ®ï¼ç°æœ‰æ–‡ä»¶ç¬¬{line_num}è¡Œä¸æŠ½æ ·æ•°æ®é‡å¤")
                        print(f"  - é‡å¤questionï¼š{existing.get('question', '')[:50]}...")
                        return True
                except json.JSONDecodeError:
                    continue
    except Exception as e:
        print(f"âš ï¸  è¯»å–ç°æœ‰QAæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{str(e)}ï¼Œè·³è¿‡é‡å¤æ£€æµ‹")
        return False
    
    return False

def save_output_files(questions_data: List[Dict[str, Any]], new_reference_records: List[Dict[str, Any]],
                     q_output_path: str, r_output_path: str):
    """ä¿å­˜ç»“æœæ–‡ä»¶"""
    with open(q_output_path, 'a+', encoding='utf-8') as f:
        for record in questions_data:
            json.dump(record, f, ensure_ascii=False, separators=(',', ':'))
            f.write('\n')
    print(f"\nQAæ–‡ä»¶å·²è¿½åŠ è‡³ï¼š{q_output_path}")
    qa_total = sum(1 for line in open(q_output_path, 'r', encoding='utf-8') if line.strip())
    print(f"QAæ–‡ä»¶å½“å‰æ€»è®°å½•æ•°ï¼š{qa_total}")

    with open(r_output_path, 'a+', encoding='utf-8') as f:
        for record in new_reference_records:
            json.dump(record, f, ensure_ascii=False)
            f.write('\n')
    print(f"å‚è€ƒæ–‡ä»¶å·²è¿½åŠ è‡³ï¼š{r_output_path}")
    ref_total = get_max_existing_r_count(r_output_path)
    print(f"å‚è€ƒæ–‡ä»¶å½“å‰æ€»è®°å½•æ•°ï¼š{ref_total}")

def main():
    parser = argparse.ArgumentParser(description="JSONLæ•°æ®å¤„ç†ï¼šæ·±åº¦è¿‡æ»¤+é‡å¤æ£€æµ‹+Rå¤„ç†")
    parser.add_argument("--input", required=True, help="åŸå§‹æ•°æ®è·¯å¾„")
    parser.add_argument("--output-questions", required=True, help="å…¨å±€QAè¾“å‡ºè·¯å¾„")
    parser.add_argument("--output-references", required=True, help="å…¨å±€Rè¾“å‡ºè·¯å¾„")
    parser.add_argument("--output-invalid", required=True, help="æ— æ•ˆæ•°æ®è¾“å‡ºè·¯å¾„")
    args = parser.parse_args()

    global input_path
    input_path = args.input

    try:
        original_data = load_original_data(args.input)
        if not original_data:
            return
        
        print("\n==================================================")
        print("ğŸ” å¼€å§‹æ·±åº¦è¿‡æ»¤æ— æ•ˆæ•°æ®ï¼ˆå…¨é“¾è·¯Answeræ£€æµ‹ + æ¨¡ç³Šè¯è¿‡æ»¤ï¼‰...")
        print("==================================================")
        filtered_data, invalid_data = filter_invalid_data(original_data)
        
        save_invalid_data(invalid_data, args.output_invalid)
        
        if not filtered_data:
            print("è­¦å‘Šï¼šè¿‡æ»¤åæ— æœ‰æ•ˆæ•°æ®ï¼Œç¨‹åºç»“æŸ")
            return
        
        existing_count = get_max_existing_r_count(args.output_references)
        start_r_id = existing_count + 1
        
        questions_data, new_reference_records = process_r_reference(filtered_data, start_r_id)
        if not questions_data:
            return
        
        print("\n==================================================")
        print("ğŸ” å¼€å§‹é‡å¤æ£€æµ‹...")
        print("==================================================")
        sample_records = sample_random_records(questions_data)
        if check_duplicate_qa(args.output_questions, sample_records):
            print("âŒ é”™è¯¯ï¼šæ•°æ®é‡å¤ï¼Œå–æ¶ˆä¿å­˜")
            return
        
        print("\n==================================================")
        print("ğŸ’¾ å¼€å§‹ä¿å­˜æœ‰æ•ˆæ–‡ä»¶...")
        print("==================================================")
        save_output_files(questions_data, new_reference_records, args.output_questions, args.output_references)
        
        print("\nâœ… å¤„ç†å®Œå…¨å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ å¤„ç†å¤±è´¥ï¼š{str(e)}")
        raise

if __name__ == "__main__":
    main()