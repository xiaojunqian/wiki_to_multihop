import os
import json
import argparse
import uuid
import glob
import re
import pandas as pd
from tqdm import tqdm

# ==========================================
# 1. ç¯å¢ƒé…ç½® (æŒ‰è¦æ±‚æŒ‡å®šæ˜¾å¡)
# ==========================================
os.environ["CUDA_VISIBLE_DEVICES"] = "6,7"

# ==========================================
# 2. è¾…åŠ©å‡½æ•°: æ®µè½æ‹†åˆ†è§„åˆ™
# ==========================================
def split_into_paragraphs(text: str):
    """
    [Splitting Rule]
    ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼å°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½ï¼Œå¹¶æ¸…ç†æ¯ä¸ªæ®µè½ã€‚
    è§„åˆ™: åœ¨å¥å­ç»“æŸç¬¦(.!?)åç´§è·Ÿæ¢è¡Œç¬¦çš„ä½ç½®è¿›è¡Œåˆ‡åˆ†ã€‚
    """
    if not text:
        return []
    
    # ä½¿ç”¨æ­£å‘åè¡Œæ–­è¨€åœ¨å¥å­ç»“æŸç¬¦å’Œæ¢è¡Œç¬¦ååˆ†å‰²
    # é€»è¾‘ï¼šæ‰¾åˆ° [.!?] åé¢ç´§è·Ÿæ¢è¡Œç¬¦çš„åœ°æ–¹è¿›è¡Œåˆ‡åˆ†
    paragraphs = re.split(r'(?<=[.!?])\s*\n+', text.strip())
    
    # æ¸…ç†æ¯ä¸ªæ®µè½ï¼Œç§»é™¤å†…éƒ¨æ¢è¡Œç¬¦å¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    return [p.strip().replace('\n', ' ') for p in paragraphs if p.strip()]

# ==========================================
# 3. æ ¸å¿ƒé€»è¾‘: æ‹†åˆ† -> æ‹¼æ¥ -> æˆªæ–­ -> è¿‡æ»¤
# ==========================================
def apply_strict_processing_logic(raw_content):
    """
    [Core Logic]
    å¤åˆ» process_single_item_task ä¸­çš„æ‹¼æ¥ä¸æˆªæ–­é€»è¾‘ã€‚
    """
    final_content = ""
    type_tag = ""
    content_list = []

    # --- æ­¥éª¤ A: ç»Ÿä¸€è½¬ä¸ºåˆ—è¡¨ (List of chunks) ---
    if isinstance(raw_content, str):
        # å¦‚æœæ˜¯å­—ç¬¦ä¸² (Wikiå…¨æ–‡)ï¼Œå…ˆåº”ç”¨æ‹†åˆ†è§„åˆ™
        content_list = split_into_paragraphs(raw_content)
        type_tag = "split_from_str"
    elif isinstance(raw_content, list):
        # å¦‚æœå·²ç»æ˜¯åˆ—è¡¨ï¼Œç›´æ¥ä½¿ç”¨
        content_list = raw_content
        type_tag = "original_list"
    else:
        # å…¶ä»–æƒ…å†µå¼ºè½¬åˆ—è¡¨
        content_list = [str(raw_content)]
        type_tag = "fallback_list"

    # --- æ­¥éª¤ B: ä¸¥æ ¼æ‹¼æ¥é€»è¾‘ (Strict Splicing) ---
    if len(content_list) > 0:
        try:
            if len(content_list) == 1:
                final_content = content_list[0]
            else:
                # === æ ¸å¿ƒå¾ªç¯ ===
                temp_i = 1
                tempstr = content_list[0]
                
                # [å…³é”®è§„åˆ™] åªæœ‰å½“å‰é•¿åº¦ < 200 æ—¶ï¼Œæ‰ç»§ç»­æ‹¼æ¥ä¸‹ä¸€æ®µ
                while temp_i < len(content_list) and len(tempstr) < 200:
                    # åŸé€»è¾‘: tempstr += item[r_key][temp_i]
                    tempstr += content_list[temp_i]
                    temp_i += 1
                
                final_content = tempstr
        except Exception:
            # å¼‚å¸¸å›é€€
            final_content = str(raw_content)[:500]
            type_tag += "_error"
    else:
        # ç©ºåˆ—è¡¨æƒ…å†µ
        return None, False, "empty_list"

    # --- æ­¥éª¤ C: æ­§ä¹‰é¡µæ£€æŸ¥ (å·²è¡¥å›) ---
    # å¯¹åº”åŸé€»è¾‘: if "may refer to:" in content: return {"error": ...}
    if "may refer to:" in final_content:
        return None, False, "disambiguation_page_filtered"

    # --- æ­¥éª¤ D: ç©ºå†…å®¹æ£€æŸ¥ ---
    if not final_content.strip():
        return None, False, "empty_content"

    return final_content, True, type_tag

# ==========================================
# 4. ä¸»æµç¨‹: éå† Parquet -> ç”Ÿæˆ Ref
# ==========================================
def process_wiki_data(wiki_data_path, output_file):
    print(f"[*] æ˜¾å¡ç¯å¢ƒ: CUDA_VISIBLE_DEVICES={os.environ.get('CUDA_VISIBLE_DEVICES')}")
    print(f"[*] Wiki æ•°æ®è·¯å¾„: {wiki_data_path}")
    print(f"[*] è¾“å‡ºæ–‡ä»¶: {output_file}")

    # 1. æ‰«æ Parquet æ–‡ä»¶
    parquet_files = sorted(glob.glob(os.path.join(wiki_data_path, "*.parquet")))
    if not parquet_files:
        print(f"âŒ åœ¨ {wiki_data_path} æœªæ‰¾åˆ° .parquet æ–‡ä»¶ï¼Œå°è¯•ç›´æ¥è¯»å–...")
        parquet_files = sorted(glob.glob(wiki_data_path)) 
    
    if not parquet_files:
        raise FileNotFoundError("æœªæ‰¾åˆ°ä»»ä½•è¾“å…¥æ–‡ä»¶ã€‚")

    print(f"[*] å…±å‘ç° {len(parquet_files)} ä¸ªæ–‡ä»¶ã€‚")

    # 2. å‡†å¤‡è¾“å‡º
    os.makedirs(os.path.dirname(output_file), exist_ok=True)
    
    # å…¨å±€å»é‡é›†åˆ (åŸºäº Wiki ID)
    seen_doc_ids = set()
    
    total_records = 0
    valid_refs = 0
    duplicate_refs = 0
    skipped_refs = 0

    # 3. é€æ–‡ä»¶å¤„ç†
    with open(output_file, 'w', encoding='utf-8') as f_out:
        for file_path in tqdm(parquet_files, desc="Processing Parquet Files"):
            try:
                # ä½¿ç”¨ PyArrow å¼•æ“è¯»å–
                df = pd.read_parquet(file_path, engine='auto')
                
                # æ£€æŸ¥å¿…è¦åˆ—
                required_cols = ['id', 'text']
                if not all(col in df.columns for col in required_cols):
                    print(f"âš ï¸ æ–‡ä»¶ {os.path.basename(file_path)} ç¼ºå°‘å¿…è¦åˆ— (id, text)ï¼Œè·³è¿‡ã€‚")
                    continue
                
                records = df.to_dict('records')
                
                for row in records:
                    total_records += 1
                    
                    # --- A. ID è·å–ä¸å»é‡ ---
                    doc_id = str(row['id']).strip()
                    if doc_id in seen_doc_ids:
                        duplicate_refs += 1
                        continue
                    
                    # --- B. æ•°æ®æå– ---
                    raw_text = row['text']
                    title = row.get('title', '').strip()
                    
                    # --- C. åº”ç”¨æ‹†åˆ†+æ‹¼æ¥+è¿‡æ»¤é€»è¾‘ ---
                    final_content, is_valid, tag = apply_strict_processing_logic(raw_text)
                    
                    if not is_valid:
                        skipped_refs += 1
                        continue
                    
                    # --- D. æ„é€  Ref å¯¹è±¡ ---
                    # [Format] Title + \n + Content (Spliced < 200 chars)
                    final_value = f"{title}\n{final_content}" if title else final_content
                    
                    ref_item = {
                        "id": str(uuid.uuid4()),
                        "value": final_value,
                        "meta_data": {
                            "document_id": doc_id,
                            "title": title,
                            "original_source": "wiki_dump",
                            "process_type": tag, 
                            "is_truncated": True 
                        }
                    }
                    
                    # --- E. å†™å…¥ä¸è®°å½• ---
                    f_out.write(json.dumps(ref_item, ensure_ascii=False) + '\n')
                    seen_doc_ids.add(doc_id)
                    valid_refs += 1
                    
            except Exception as e:
                print(f"âŒ å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‡ºé”™: {e}")

    print("\n" + "="*40)
    print("ğŸ‰ å¤„ç†å®Œæˆ (Processing Complete)")
    print(f"ğŸ“Š æ€»æ‰«ææ–‡æ¡£: {total_records}")
    print(f"âœ… ç”Ÿæˆ Ref æ•°: {valid_refs}")
    print(f"â­ï¸  å…¨å±€å»é‡æ•°: {duplicate_refs}")
    print(f"ğŸ—‘ï¸  æ— æ•ˆ/æ­§ä¹‰æ•°: {skipped_refs}")
    print(f"ğŸ“‚ ç»“æœä¿å­˜è‡³: {output_file}")
    print("="*40)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Wiki Ref ç”Ÿæˆå™¨ (æ‹†åˆ†+ä¸¥æ ¼æ‹¼æ¥+æ¶ˆæ­§ä¹‰ç‰ˆ)")
    
    parser.add_argument("--wiki_data_path", type=str, required=True, 
                        help="åŒ…å« .parquet æ–‡ä»¶çš„ç›®å½•è·¯å¾„")
    parser.add_argument("--output", type=str, required=True, 
                        help="è¾“å‡º .jsonl æ–‡ä»¶çš„å®Œæ•´è·¯å¾„")
    
    args = parser.parse_args()
    
    process_wiki_data(args.wiki_data_path, args.output)